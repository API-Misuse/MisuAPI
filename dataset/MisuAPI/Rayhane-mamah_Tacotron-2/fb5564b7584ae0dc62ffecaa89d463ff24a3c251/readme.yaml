sha: fb5564b7584ae0dc62ffecaa89d463ff24a3c251
message: my hparams + faster attention + pitch loss fix
commit_log: "commit fb5564b7584ae0dc62ffecaa89d463ff24a3c251\nAuthor: Rayhane Mama\
  \ <34689728+Rayhane-mamah@users.noreply.github.com>\nDate:   Fri May 25 15:27:31\
  \ 2018 +0100\n\n    my hparams + faster attention + pitch loss fix"
contained_keywords:
- fix
commit_time: 2018-05-25 15:27:31 +0100
commit_author: Rayhane Mama
added_files: 0
removed_files: 0
modified_files: 9
patched_file_0:
  is_added_file: false
  is_removed_file: false
  is_modified_file: true
  is_binary_file: false
  is_rename: false
  path: datasets/audio.py
  source_file: a/datasets/audio.py
  target_file: b/datasets/audio.py
  added: 8
  removed: 8
  add_line_no:
  - - 36
    - "\t#Thanks @begeekmyfriend and @lautjy for pointing out the params contradiction.\
      \ These params are separate and tunable per dataset.\n"
  - - 37
    - "\treturn librosa.effects.trim(wav, top_db= hparams.trim_top_db, frame_length=hparams.trim_fft_size,\
      \ hop_length=hparams.trim_hop_size)[0]\n"
  - - 77
    - "\t\treturn _griffin_lim(S ** hparams.power, hparams)\n"
  - - 95
    - "\t\treturn _griffin_lim(S ** hparams.power, hparams)\n"
  - - 99
    - "\treturn lws.lws(hparams.n_fft, get_hop_size(hparams), fftsize=hparams.win_size,\
      \ mode=\"speech\")\n"
  - - 117
    - "\t\treturn librosa.stft(y=y, n_fft=hparams.n_fft, hop_length=get_hop_size(hparams),\
      \ win_length=hparams.win_size)\n"
  - - 120
    - "\treturn librosa.istft(y, hop_length=get_hop_size(hparams), win_length=hparams.win_size)\n"
  - - 161
    - "\treturn librosa.filters.mel(hparams.sample_rate, hparams.n_fft, n_mels=hparams.num_mels,\n"
  del_line_no:
  - - null
    - "\t#Thanks @begeekmyfriend for pointing out the params contradiction. Should\
      \ we set separate params for this function?\n"
  - - null
    - "\treturn librosa.effects.trim(wav, frame_length=hparams.fft_size, hop_length=get_hop_size(hparams))[0]\n"
  - - null
    - "\t\treturn _griffin_lim(S ** hparams.power)\n"
  - - null
    - "\t\treturn _griffin_lim(S ** hparams.power)\n"
  - - null
    - "\treturn lws.lws(hparams.fft_size, get_hop_size(hparams), mode=\"speech\")\n"
  - - null
    - "\t\treturn librosa.stft(y=y, n_fft=hparams.fft_size, hop_length=get_hop_size(hparams))\n"
  - - null
    - "\treturn librosa.istft(y, hop_length=get_hop_size(hparams))\n"
  - - null
    - "\treturn librosa.filters.mel(hparams.sample_rate, hparams.fft_size, n_mels=hparams.num_mels,\n"
patched_file_1:
  is_added_file: false
  is_removed_file: false
  is_modified_file: true
  is_binary_file: false
  is_rename: false
  path: datasets/preprocessor.py
  source_file: a/datasets/preprocessor.py
  target_file: b/datasets/preprocessor.py
  added: 5
  removed: 4
  add_line_no:
  - - 118
    - "\tfft_size = hparams.n_fft if hparams.win_size is None else hparams.win_size\n"
  - - 119
    - "\tl, r = audio.pad_lr(wav, fft_size, audio.get_hop_size(hparams))\n"
  - - 123
    - "\tassert len(out) >= mel_frames * audio.get_hop_size(hparams)\n"
  - - 129
    - "\tassert len(out) % audio.get_hop_size(hparams) == 0\n"
  - - 130
    - "\ttime_steps = len(out)\n"
  del_line_no:
  - - null
    - "\tl, r = audio.pad_lr(wav, hparams.fft_size, audio.get_hop_size(hparams))\n"
  - - null
    - "\ttime_steps = len(out)\n"
  - - null
    - "\tassert time_steps >= mel_frames * audio.get_hop_size(hparams)\n"
  - - null
    - "\tassert time_steps % audio.get_hop_size(hparams) == 0\n"
patched_file_2:
  is_added_file: false
  is_removed_file: false
  is_modified_file: true
  is_binary_file: false
  is_rename: false
  path: hparams.py
  source_file: a/hparams.py
  target_file: b/hparams.py
  added: 24
  removed: 17
  add_line_no:
  - - 18
    - "\tnum_freq = 1025, # (= n_fft / 2 + 1) only used when adding linear spectrograms\
      \ post processing network\n"
  - - 27
    - "\t# Does not work if n_ffit is not multiple of hop_size!!\n"
  - - 28
    - "\tuse_lws=False,\n"
  - - 32
    - "\tn_fft = 2048, #Extra window size is filled with 0 paddings to match this\
      \ parameter\n"
  - - 33
    - "\thop_size = 275, #For 22050Hz, 275 ~= 12.5 ms\n"
  - - 34
    - "\twin_size = 1100, #For 22050Hz, 1100 ~= 50 ms (If None, win_size = n_fft)\n"
  - - 38
    - "\t#M-AILABS (and other datasets) trim params\n"
  - - 39
    - "\ttrim_fft_size = 512,\n"
  - - 40
    - "\ttrim_hop_size = 128,\n"
  - - 41
    - "\ttrim_top_db = 60,\n"
  - - 42
    - '

      '
  - - 50
    - "\tmin_level_db = -100,\n"
  - - 52
    - "\tfmin = 25, #Set this to 75 if your speaker is male! if female, 125 should\
      \ help taking off noise. (To test depending on dataset)\n"
  - - 77
    - "\tprenet_layers = [128, 128], #number of layers and number of units of prenet\n"
  - - 79
    - "\tdecoder_lstm_units = 512, #number of decoder lstm units on each layer\n"
  - - 103
    - "\tquantize_channels=65536,  # 65536 (16-bit) (raw) or 256 (8-bit) (mulaw or\
      \ mulaw-quantize) // number of classes = 256 <=> mu = 255\n"
  - - 117
    - "\tupsample_scales = [11, 5, 5], #prod(scales) should be equal to hop size\n"
  - - 131
    - "\ttacotron_batch_size = 48, #number of training samples on each training steps\n"
  - - 132
    - "\ttacotron_reg_weight = 1e-5, #regularization weight (for L2 regularization)\n"
  - - 136
    - "\ttacotron_test_batches = 32, #number of test batches (For Ljspeech: 10% ~=\
      \ 41 batches of 32 samples)\n"
  - - 152
    - '

      '
  - - 153
    - "\tnatural_eval = False, #Whether to use 100% natural eval (to evaluate Curriculum\
      \ Learning performance) or with same teacher-forcing ratio as in training (just\
      \ for overfit)\n"
  - - 166
    - "\ttacotron_teacher_forcing_decay_steps = 280000, #Determines the teacher forcing\
      \ ratio decay slope. Relevant if mode='scheduled'\n"
  - - 236
    - "\treturn 'Hyperparameters:\\n' + '\\n'.join(hp)\n"
  del_line_no:
  - - null
    - "\tnum_freq = 513, #only used when adding linear spectrograms post processing\
      \ network\n"
  - - null
    - "\tuse_lws=True,\n"
  - - null
    - "\tfft_size = 1024,\n"
  - - null
    - "\thop_size = 256,\n"
  - - null
    - "\tmin_level_db =- 100,\n"
  - - null
    - "\tfmin = 125, #Set this to 75 if your speaker is male! if female, 125 should\
      \ help taking off noise. (To test depending on dataset)\n"
  - - null
    - "\tprenet_layers = [256, 256], #number of layers and number of units of prenet\n"
  - - null
    - "\tdecoder_lstm_units = 1024, #number of decoder lstm units on each layer\n"
  - - null
    - "\tquantize_channels=65536,  # 65536 (raw) or 256 (mulaw or mulaw-quantize)\
      \ // number of classes = 256 <=> mu = 255\n"
  - - null
    - "\tupsample_scales = [16, 16], #prod(scales) should be equal to hop size\n"
  - - null
    - "\ttacotron_batch_size = 32, #number of training samples on each training steps\n"
  - - null
    - "\ttacotron_reg_weight = 1e-6, #regularization weight (for l2 regularization)\n"
  - - null
    - "\ttacotron_test_batches = 48, #number of test batches (For Ljspeech: 10% ~=\
      \ 41 batches of 32 samples)\n"
  - - null
    - "\t\n"
  - - null
    - "\tnatural_eval = True, #Whether to use 100% natural eval (to evaluate Curriculum\
      \ Learning performance) or with same teacher-forcing ratio as training (just\
      \ for overfit)\n"
  - - null
    - "\ttacotron_teacher_forcing_decay_steps = 220000, #Determines the teacher forcing\
      \ ratio decay slope. Relevant if mode='scheduled'\n"
  - - null
    - "\treturn 'Hyperparameters:\\n' + '\\n'.join(hp)\n"
patched_file_3:
  is_added_file: false
  is_removed_file: false
  is_modified_file: true
  is_binary_file: false
  is_rename: false
  path: synthesize.py
  source_file: a/synthesize.py
  target_file: b/synthesize.py
  added: 2
  removed: 1
  add_line_no:
  - - 6
    - 'from warnings import warn

      '
  - - 73
    - "\t\t\twarn('Requested a live evaluation with Tacotron-2, Wavenet will not be\
      \ used!')\n"
  del_line_no:
  - - null
    - "\t\t\traise ValueError('Impossible to generate live speech using Tacotron+WaveNet\
      \ combo! (only eval allowed)')\n"
patched_file_4:
  is_added_file: false
  is_removed_file: false
  is_modified_file: true
  is_binary_file: false
  is_rename: false
  path: tacotron/models/attention.py
  source_file: a/tacotron/models/attention.py
  target_file: b/tacotron/models/attention.py
  added: 2
  removed: 2
  add_line_no:
  - - 162
    - "\t\t\tkernel_size=hparams.attention_kernel, padding='same', use_bias=True,\n"
  - - 163
    - "\t\t\tbias_initializer=tf.zeros_initializer(), name='location_features_convolution')\n"
  del_line_no:
  - - null
    - "\t\t\tkernel_size=hparams.attention_kernel, padding='same', use_bias=False,\
      \ \n"
  - - null
    - "\t\t\tname='location_features_convolution')\n"
patched_file_5:
  is_added_file: false
  is_removed_file: false
  is_modified_file: true
  is_binary_file: false
  is_rename: false
  path: tacotron/models/tacotron.py
  source_file: a/tacotron/models/tacotron.py
  target_file: b/tacotron/models/tacotron.py
  added: 1
  removed: 1
  add_line_no:
  - - 267
    - "\t\t\tclipped_gradients, _ = tf.clip_by_global_norm(gradients, 1.)\n"
  del_line_no:
  - - null
    - "\t\t\tclipped_gradients, _ = tf.clip_by_global_norm(gradients, 0.5)\n"
patched_file_6:
  is_added_file: false
  is_removed_file: false
  is_modified_file: true
  is_binary_file: false
  is_rename: false
  path: tacotron/train.py
  source_file: a/tacotron/train.py
  target_file: b/tacotron/train.py
  added: 1
  removed: 1
  add_line_no:
  - - 179
    - "\t\t\t\tif np.isnan(loss):\n"
  del_line_no:
  - - null
    - "\t\t\t\tif loss > 1000 or np.isnan(loss):\n"
patched_file_7:
  is_added_file: false
  is_removed_file: false
  is_modified_file: true
  is_binary_file: false
  is_rename: false
  path: train.py
  source_file: a/train.py
  target_file: b/train.py
  added: 4
  removed: 4
  add_line_no:
  - - 56
    - "\tparser.add_argument('--checkpoint_interval', type=int, default=5000,\n"
  - - 58
    - "\tparser.add_argument('--eval_interval', type=int, default=10000,\n"
  - - 60
    - "\tparser.add_argument('--tacotron_train_steps', type=int, default=160000, help='total\
      \ number of tacotron training steps')\n"
  - - 83
    - "\tmain()\n"
  del_line_no:
  - - null
    - "\tparser.add_argument('--checkpoint_interval', type=int, default=1000,\n"
  - - null
    - "\tparser.add_argument('--eval_interval', type=int, default=5000,\n"
  - - null
    - "\tparser.add_argument('--tacotron_train_steps', type=int, default=320000, help='total\
      \ number of tacotron training steps')\n"
  - - null
    - "\tmain()\n"
patched_file_8:
  is_added_file: false
  is_removed_file: false
  is_modified_file: true
  is_binary_file: false
  is_rename: false
  path: wavenet_vocoder/models/modules.py
  source_file: a/wavenet_vocoder/models/modules.py
  target_file: b/wavenet_vocoder/models/modules.py
  added: 50
  removed: 47
  add_line_no:
  - - 57
    - "\t\t\tself.scope = scope\n"
  - - 122
    - "\t\twith tf.variable_scope(self.scope):\n"
  - - 123
    - "\t\t\t#Reshape to dilated conv mode (if this instance is of a dilated convolution)\n"
  - - 124
    - "\t\t\tinputs_ = self._to_dilation(inputs)\n"
  - - 126
    - "\t\t\toutputs_ = tf.nn.conv1d(inputs_, self.kernel,\n"
  - - 127
    - "\t\t\t\tstride=1, padding='VALID', data_format='NWC')\n"
  - - 129
    - "\t\t\tif self.use_bias:\n"
  - - 130
    - "\t\t\t\toutputs_ = tf.nn.bias_add(outputs_, self.bias)\n"
  - - 132
    - "\t\t\t#Reshape back ((if this instance is of a dilated convolution))\n"
  - - 133
    - "\t\t\tdiff = tf.shape(outputs_)[1] * self.dilation_rate - tf.shape(inputs)[-1]\n"
  - - 134
    - "\t\t\toutputs = self._from_dilation(outputs_, crop=diff)\n"
  - - 136
    - "\t\t\t#Make sure that outputs have same time steps as inputs\n"
  - - 137
    - "\t\t\t#[batch_size, channels(filters), width]\n"
  - - 138
    - "\t\t\twith tf.control_dependencies([tf.assert_equal(tf.shape(outputs)[-1],\
      \ tf.shape(inputs)[-1])]):\n"
  - - 139
    - "\t\t\t\toutputs = tf.identity(outputs, name='output_equal_input_time_assert')\n"
  - - 141
    - "\t\t\treturn outputs\n"
  - - 149
    - "\t\twith tf.variable_scope(self.scope):\n"
  - - 150
    - "\t\t\t#input: [batch_size, time_length, channels]\n"
  - - 151
    - "\t\t\tif self.training: \n"
  - - 152
    - "\t\t\t\traise RuntimeError('incremental_step only supports eval mode')\n"
  - - 153
    - '

      '
  - - 154
    - "\t\t\t#reshape weight\n"
  - - 155
    - "\t\t\tweight = self._get_linearized_weight(inputs)\n"
  - - 156
    - "\t\t\tkw = self.kernel.shape[0]\n"
  - - 157
    - "\t\t\tdilation = self.dilation_rate\n"
  - - 158
    - '

      '
  - - 159
    - "\t\t\tbatch_size = tf.shape(inputs)[0]\n"
  - - 160
    - "\t\t\t#Fast dilation\n"
  - - 161
    - "\t\t\t#Similar to using tf FIFOQueue to schedule states of dilated convolutions\n"
  - - 162
    - "\t\t\tif kw > 1:\n"
  - - 163
    - "\t\t\t\tif self.convolution_queue is None:\n"
  - - 164
    - "\t\t\t\t\tself.convolution_queue = tf.zeros((batch_size, (kw - 1) + (kw - 1)\
      \ * (dilation - 1), tf.shape(inputs)[2]))\n"
  - - 165
    - "\t\t\t\telse:\n"
  - - 166
    - "\t\t\t\t\t#shift queue\n"
  - - 167
    - "\t\t\t\t\tself.convolution_queue = self.convolution_queue[:, 1:, :]\n"
  - - 168
    - '

      '
  - - 169
    - "\t\t\t\t#append next input\n"
  - - 170
    - "\t\t\t\tself.convolution_queue = tf.concat([self.convolution_queue, tf.expand_dims(inputs[:,\
      \ -1, :], axis=1)], axis=1)\n"
  - - 171
    - "\t\t\t\t#self.convolution_queue[:, -1, :] = inputs[:, -1, :]\n"
  - - 172
    - "\t\t\t\tinputs = self.convolution_queue\n"
  - - 173
    - "\t\t\t\tif dilation > 1:\n"
  - - 174
    - "\t\t\t\t\tinputs = inputs[:, 0::dilation, :]\n"
  - - 175
    - '

      '
  - - 176
    - "\t\t\t#Compute step prediction\n"
  - - 177
    - "\t\t\toutput = tf.matmul(tf.reshape(inputs, [batch_size, -1]), weight)\n"
  - - 178
    - "\t\t\tif self.use_bias:\n"
  - - 179
    - "\t\t\t\toutput = tf.nn.bias_add(output, self.bias)\n"
  - - 180
    - '

      '
  - - 181
    - "\t\t\t#[batch_size, 1(time_step), channels(filters)]\n"
  - - 182
    - "\t\t\treturn tf.reshape(output, [batch_size, 1, self.filters])\n"
  del_line_no:
  - - null
    - "\t\t#Reshape to dilated conv mode (if this instance is of a dilated convolution)\n"
  - - null
    - "\t\tinputs_ = self._to_dilation(inputs)\n"
  - - null
    - "\t\toutputs_ = tf.nn.conv1d(inputs_, self.kernel,\n"
  - - null
    - "\t\t\tstride=1, padding='VALID', data_format='NWC')\n"
  - - null
    - "\t\tif self.use_bias:\n"
  - - null
    - "\t\t\toutputs_ = tf.nn.bias_add(outputs_, self.bias)\n"
  - - null
    - "\t\t#Reshape back ((if this instance is of a dilated convolution))\n"
  - - null
    - "\t\tdiff = tf.shape(outputs_)[1] * self.dilation_rate - tf.shape(inputs)[-1]\n"
  - - null
    - "\t\toutputs = self._from_dilation(outputs_, crop=diff)\n"
  - - null
    - "\t\t#Make sure that outputs have same time steps as inputs\n"
  - - null
    - "\t\t#[batch_size, channels(filters), width]\n"
  - - null
    - "\t\twith tf.control_dependencies([tf.assert_equal(tf.shape(outputs)[-1], tf.shape(inputs)[-1])]):\n"
  - - null
    - "\t\t\toutputs = tf.identity(outputs, name='output_equal_input_time_assert')\n"
  - - null
    - "\t\treturn outputs\n"
  - - null
    - "\t\t#input: [batch_size, time_length, channels]\n"
  - - null
    - "\t\tif self.training: \n"
  - - null
    - "\t\t\traise RuntimeError('incremental_step only supports eval mode')\n"
  - - null
    - '

      '
  - - null
    - "\t\t#reshape weight\n"
  - - null
    - "\t\tweight = self._get_linearized_weight(inputs)\n"
  - - null
    - "\t\tkw = self.kernel.shape[0]\n"
  - - null
    - "\t\tdilation = self.dilation_rate\n"
  - - null
    - '

      '
  - - null
    - "\t\tbatch_size = tf.shape(inputs)[0]\n"
  - - null
    - "\t\t#Fast dilation\n"
  - - null
    - "\t\t#Similar to using tf FIFOQueue to schedule states of dilated convolutions\n"
  - - null
    - "\t\tif kw > 1:\n"
  - - null
    - "\t\t\tif self.convolution_queue is None:\n"
  - - null
    - "\t\t\t\tself.convolution_queue = tf.zeros((batch_size, (kw - 1) + (kw - 1)\
      \ * (dilation - 1), tf.shape(inputs)[2]))\n"
  - - null
    - "\t\t\telse:\n"
  - - null
    - "\t\t\t\t#shift queue\n"
  - - null
    - "\t\t\t\tself.convolution_queue = self.convolution_queue[:, 1:, :]\n"
  - - null
    - '

      '
  - - null
    - "\t\t\t#append next input\n"
  - - null
    - "\t\t\tself.convolution_queue = tf.concat([self.convolution_queue, tf.expand_dims(inputs[:,\
      \ -1, :], axis=1)], axis=1)\n"
  - - null
    - "\t\t\t#self.convolution_queue[:, -1, :] = inputs[:, -1, :]\n"
  - - null
    - "\t\t\tinputs = self.convolution_queue\n"
  - - null
    - "\t\t\tif dilation > 1:\n"
  - - null
    - "\t\t\t\tinputs = inputs[:, 0::dilation, :]\n"
  - - null
    - '

      '
  - - null
    - "\t\t#Compute step prediction\n"
  - - null
    - "\t\toutput = tf.matmul(tf.reshape(inputs, [batch_size, -1]), weight)\n"
  - - null
    - "\t\tif self.use_bias:\n"
  - - null
    - "\t\t\toutput = tf.nn.bias_add(output, self.bias)\n"
  - - null
    - '

      '
  - - null
    - "\t\t#[batch_size, 1(time_step), channels(filters)]\n"
  - - null
    - "\t\treturn tf.reshape(output, [batch_size, 1, self.filters])\n"
