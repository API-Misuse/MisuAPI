commit 6af52be48d789e7f0226d9d660acbf19e60f8538
Author: iamsusiep <sujip@google.com>
Date:   Thu Jul 30 04:30:06 2020 -0400

    raise error if component id or name doesn't exist

diff --git a/tfx/experimental/pipeline_testing/pipeline_recorder_utils.py b/tfx/experimental/pipeline_testing/pipeline_recorder_utils.py
index 36e64aad..7f4b635a 100644
--- a/tfx/experimental/pipeline_testing/pipeline_recorder_utils.py
+++ b/tfx/experimental/pipeline_testing/pipeline_recorder_utils.py
@@ -20,22 +20,23 @@ from __future__ import print_function
 
 import collections
 import os
-from typing import Iterable, List, Mapping, Optional, Text, Tuple
 
 from absl import logging
+from ml_metadata.proto import metadata_store_pb2
 import tensorflow as tf
+from typing import Iterable, List, Mapping, Optional, Text, Tuple
+
 from tfx.orchestration import metadata
 from tfx.utils import io_utils
 
-from ml_metadata.proto import metadata_store_pb2
-
-
-def _get_paths(metadata_connection: metadata.Metadata, execution_ids: List[int],
+def _get_paths(metadata_connection: metadata.Metadata,
+               execution_ids: List[int],
                output_dir: Text) -> Iterable[Tuple[Text, Text]]:
   """Yields tuple with source and destination artifact uris.
 
   The destination artifact uris are located in the output_dir. The source
-  artifact uris are retrieved using execution ids.
+  artifact uris are retrieved using execution ids. If the artifact name is
+  missing, the default name would be set to {component_id}_name.
 
   Args:
     metadata_connection: Instance of metadata.Metadata for I/O to MLMD.
@@ -45,9 +46,11 @@ def _get_paths(metadata_connection: metadata.Metadata, execution_ids: List[int],
   Yields:
     Iterable over tuples of source uri and destination uri.
   """
-  events = metadata_connection.store.get_events_by_execution_ids(execution_ids)
+  events = metadata_connection.store.get_events_by_execution_ids(
+      execution_ids)
   output_events = [
-      x for x in events if x.type == metadata_store_pb2.Event.OUTPUT
+      x for x in events
+      if x.type == metadata_store_pb2.Event.OUTPUT
   ]
   unique_artifact_ids = list({x.artifact_id for x in output_events})
 
@@ -56,14 +59,14 @@ def _get_paths(metadata_connection: metadata.Metadata, execution_ids: List[int],
     src_uri = artifact.uri
     artifact_properties = artifact.custom_properties
     component_id = artifact_properties['producer_component'].string_value
-    name = artifact_properties['name'].string_value
-    dest_uri = os.path.join(output_dir, component_id, name)
+    output_key = artifact_properties['name'].string_value
+    if not component_id or not output_key:
+      raise ValueError("component_id and output_key cannot be None.")
+    dest_uri = os.path.join(output_dir, component_id, output_key)
     yield (src_uri, dest_uri)
 
-
-def _get_execution_dict(
-    metadata_connection: metadata.Metadata
-) -> Mapping[Text, List[metadata_store_pb2.Execution]]:
+def _get_execution_dict(metadata_connection: metadata.Metadata
+                       ) -> Mapping[Text, List[metadata_store_pb2.Execution]]:
   """Returns a dictionary holding list of executions for all run_id in MLMD.
 
   Args:
@@ -78,10 +81,9 @@ def _get_execution_dict(
     execution_dict[execution_run_id].append(execution)
   return execution_dict
 
-
-def _get_latest_executions(
-    metadata_connection: metadata.Metadata,
-    pipeline_name: Text) -> List[metadata_store_pb2.Execution]:
+def _get_latest_executions(metadata_connection: metadata.Metadata,
+                           pipeline_name: Text
+                           ) -> List[metadata_store_pb2.Execution]:
   """Fetches executions associated with the latest context.
 
   Args:
@@ -97,13 +99,14 @@ def _get_latest_executions(
           metadata._CONTEXT_TYPE_PIPELINE_RUN)  # pylint: disable=protected-access
       if c.properties['pipeline_name'].string_value == pipeline_name
   ]
-  latest_context = max(
-      pipeline_run_contexts, key=lambda c: c.last_update_time_since_epoch)
+  latest_context = max(pipeline_run_contexts,
+                       key=lambda c: c.last_update_time_since_epoch)
   return metadata_connection.store.get_executions_by_context(latest_context.id)
 
-
-def record_pipeline(output_dir: Text, metadata_db_uri: Optional[Text],
-                    host: Optional[Text], port: Optional[int],
+def record_pipeline(output_dir: Text,
+                    metadata_db_uri: Optional[Text],
+                    host: Optional[Text],
+                    port: Optional[int],
                     pipeline_name: Optional[Text],
                     run_id: Optional[Text]) -> None:
   """Record pipeline run with run_id to output_dir.
@@ -143,18 +146,20 @@ def record_pipeline(output_dir: Text, metadata_db_uri: Optional[Text],
         raise ValueError('If the run_id is not specified,'
                          ' pipeline_name should be specified')
       # fetch executions of the most recently updated execution context.
-      executions = _get_latest_executions(metadata_connection, pipeline_name)
+      executions = _get_latest_executions(metadata_connection,
+                                          pipeline_name)
     else:
       execution_dict = _get_execution_dict(metadata_connection)
       if run_id in execution_dict:
         executions = execution_dict[run_id]
       else:
         raise ValueError(
-            'run_id {} is not recorded in the MLMD metadata'.format(run_id))
+            "run_id {} is not recorded in the MLMD metadata".format(run_id))
     execution_ids = [e.id for e in executions]
-    for src_uri, dest_uri in _get_paths(metadata_connection, execution_ids,
+    for src_uri, dest_uri in _get_paths(metadata_connection,
+                                        execution_ids,
                                         output_dir):
       if not tf.io.gfile.exists(src_uri):
-        raise FileNotFoundError('{} does not exist'.format(src_uri))
+        raise FileNotFoundError("{} does not exist".format(src_uri))
       io_utils.copy_dir(src_uri, dest_uri)
-    logging.info('Pipeline Recorded at %s', output_dir)
+    logging.info("Pipeline Recorded at %s", output_dir)
