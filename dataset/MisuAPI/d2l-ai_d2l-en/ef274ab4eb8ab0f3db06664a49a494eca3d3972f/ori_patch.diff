commit ef274ab4eb8ab0f3db06664a49a494eca3d3972f
Author: Aston Zhang <asv325@gmail.com>
Date:   Tue Sep 1 17:58:31 2020 +0000

    +improve Vocab, fix nli bert

diff --git a/chapter_natural-language-processing-applications/natural-language-inference-bert.md b/chapter_natural-language-processing-applications/natural-language-inference-bert.md
index c4a1cdfc..8e0d9eeb 100644
--- a/chapter_natural-language-processing-applications/natural-language-inference-bert.md
+++ b/chapter_natural-language-processing-applications/natural-language-inference-bert.md
@@ -59,7 +59,7 @@ def load_pretrained_model(pretrained_model, num_hiddens, ffn_num_hiddens,
                           num_heads, num_layers, dropout, max_len, devices):
     data_dir = d2l.download_extract(pretrained_model)
     # Define an empty vocabulary to load the predefined vocabulary
-    vocab = d2l.Vocab([])
+    vocab = d2l.Vocab()
     vocab.idx_to_token = json.load(open(os.path.join(data_dir, 'vocab.json')))
     vocab.token_to_idx = {token: idx for idx, token in enumerate(
         vocab.idx_to_token)}
diff --git a/chapter_recurrent-neural-networks/text-preprocessing.md b/chapter_recurrent-neural-networks/text-preprocessing.md
index d619801c..43b4499e 100644
--- a/chapter_recurrent-neural-networks/text-preprocessing.md
+++ b/chapter_recurrent-neural-networks/text-preprocessing.md
@@ -113,9 +113,11 @@ We optionally add a list of reserved tokens, such as
 #@tab all
 class Vocab:  #@save
     """Vocabulary for text."""
-    def __init__(self, tokens, min_freq=0, reserved_tokens=None):
+    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):
+        if tokens is None:
+            tokens = []
         if reserved_tokens is None:
-            reserved_tokens = []
+            reserved_tokens = [] 
         # Sort according to frequencies
         counter = count_corpus(tokens)
         self.token_freqs = sorted(counter.items(), key=lambda x: x[0])
@@ -144,7 +146,8 @@ class Vocab:  #@save
 
 def count_corpus(tokens):  #@save
     """Count token frequencies."""
-    if isinstance(tokens[0], list):
+    # Here `tokens` is a 1D list or 2D list
+    if len(tokens) == 0 or isinstance(tokens[0], list):
         # Flatten a list of token lists into a list of tokens
         tokens = [token for line in tokens for token in line]
     return collections.Counter(tokens)
diff --git a/d2l/mxnet.py b/d2l/mxnet.py
index 8c6a63d5..b1b79333 100644
--- a/d2l/mxnet.py
+++ b/d2l/mxnet.py
@@ -529,9 +529,11 @@ def tokenize(lines, token='word'):  #@save
 # Defined in file: ./chapter_recurrent-neural-networks/text-preprocessing.md
 class Vocab:  #@save
     """Vocabulary for text."""
-    def __init__(self, tokens, min_freq=0, reserved_tokens=None):
+    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):
+        if tokens is None:
+            tokens = []
         if reserved_tokens is None:
-            reserved_tokens = []
+            reserved_tokens = [] 
         # Sort according to frequencies
         counter = count_corpus(tokens)
         self.token_freqs = sorted(counter.items(), key=lambda x: x[0])
@@ -562,7 +564,8 @@ class Vocab:  #@save
 # Defined in file: ./chapter_recurrent-neural-networks/text-preprocessing.md
 def count_corpus(tokens):  #@save
     """Count token frequencies."""
-    if isinstance(tokens[0], list):
+    # Here `tokens` is a 1D list or 2D list
+    if len(tokens) == 0 or isinstance(tokens[0], list):
         # Flatten a list of token lists into a list of tokens
         tokens = [token for line in tokens for token in line]
     return collections.Counter(tokens)
diff --git a/d2l/tensorflow.py b/d2l/tensorflow.py
index 751053e8..c2d2a2c5 100644
--- a/d2l/tensorflow.py
+++ b/d2l/tensorflow.py
@@ -553,9 +553,11 @@ def tokenize(lines, token='word'):  #@save
 # Defined in file: ./chapter_recurrent-neural-networks/text-preprocessing.md
 class Vocab:  #@save
     """Vocabulary for text."""
-    def __init__(self, tokens, min_freq=0, reserved_tokens=None):
+    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):
+        if tokens is None:
+            tokens = []
         if reserved_tokens is None:
-            reserved_tokens = []
+            reserved_tokens = [] 
         # Sort according to frequencies
         counter = count_corpus(tokens)
         self.token_freqs = sorted(counter.items(), key=lambda x: x[0])
@@ -586,7 +588,8 @@ class Vocab:  #@save
 # Defined in file: ./chapter_recurrent-neural-networks/text-preprocessing.md
 def count_corpus(tokens):  #@save
     """Count token frequencies."""
-    if isinstance(tokens[0], list):
+    # Here `tokens` is a 1D list or 2D list
+    if len(tokens) == 0 or isinstance(tokens[0], list):
         # Flatten a list of token lists into a list of tokens
         tokens = [token for line in tokens for token in line]
     return collections.Counter(tokens)
diff --git a/d2l/torch.py b/d2l/torch.py
index 2ab05145..9c5d477f 100644
--- a/d2l/torch.py
+++ b/d2l/torch.py
@@ -560,9 +560,11 @@ def tokenize(lines, token='word'):  #@save
 # Defined in file: ./chapter_recurrent-neural-networks/text-preprocessing.md
 class Vocab:  #@save
     """Vocabulary for text."""
-    def __init__(self, tokens, min_freq=0, reserved_tokens=None):
+    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):
+        if tokens is None:
+            tokens = []
         if reserved_tokens is None:
-            reserved_tokens = []
+            reserved_tokens = [] 
         # Sort according to frequencies
         counter = count_corpus(tokens)
         self.token_freqs = sorted(counter.items(), key=lambda x: x[0])
@@ -593,7 +595,8 @@ class Vocab:  #@save
 # Defined in file: ./chapter_recurrent-neural-networks/text-preprocessing.md
 def count_corpus(tokens):  #@save
     """Count token frequencies."""
-    if isinstance(tokens[0], list):
+    # Here `tokens` is a 1D list or 2D list
+    if len(tokens) == 0 or isinstance(tokens[0], list):
         # Flatten a list of token lists into a list of tokens
         tokens = [token for line in tokens for token in line]
     return collections.Counter(tokens)
